{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"13Urhygw-veXp6ledyVmSpLxvj__Pj6MZ","authorship_tag":"ABX9TyMYGB42HU5NqnHBul5iIJo7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dRNooNZLmb_s","colab_type":"code","colab":{}},"source":["from __future__ import division\n","import numpy as np\n","import os\n","import glob\n","\n","from random import *\n","from PIL import Image\n","from keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import matplotlib.image as mpimg\n","\n","from keras.backend import squeeze\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Lambda, ELU, Activation, BatchNormalization, Bidirectional\n","from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D\n","from keras.layers.recurrent import LSTM, StackedRNNCells\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import SGD, Adam, RMSprop\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qc6yoSs7nHx-","colab_type":"code","colab":{}},"source":["d = {}\n","from subprocess import check_output\n","with open('gdrive/My Drive/Words.txt') as f:\n","    for line in f:\n","        imagename = line.split(' ')[0]\n","        wordInImage = line.split(' ')[1]\n","        d[imagename] = wordInImage"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejwtygxyocrU","colab_type":"code","colab":{}},"source":["tmp = []\n","target_list = []\n","\n","path_to_files = os.path.join('gdrive/My Drive/word test', '*')\n","for filename in sorted(glob.glob(path_to_files)):\n","  tmp.append(filename)\n","  image_name = filename.split('/')[-1]\n","  file, ext = os.path.splitext(image_name)\n","  parts = file.split('.')\n","  word = parts[0]\n","  for key in d:\n","    if key == word:\n","      target_list.append(str(d[word]))\n","\n","img_files = np.asarray(tmp)\n","img_targets = np.asarray(target_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_s9kAp63NkW","colab_type":"code","colab":{}},"source":["train_files, rem_files, train_targets, rem_targets = train_test_split(\n","        img_files, img_targets, train_size=0.66, random_state=52, shuffle= True)\n","\n","validation_files, test_files, validation_targets, test_targets = train_test_split(\n","        rem_files, rem_targets, train_size=0.5, random_state=22, shuffle=True)\n","\n","print(train_files.shape, validation_files.shape, test_files.shape)\n","print(train_targets.shape, validation_targets.shape, test_targets.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xh6wvP6c3N1T","colab_type":"code","colab":{}},"source":["batch_size = 20\n","num_classes = 50\n","\n","def generate_data(samples, target_files,  batch_size=batch_size, factor = 0.1 ):\n","  num_samples = len(samples)\n","  from sklearn.utils import shuffle\n","  while 1: # Loop forever so the generator never terminates\n","        for offset in range(0, num_samples, batch_size):\n","            batch_samples = samples[offset:offset+batch_size]\n","            batch_targets = target_files[offset:offset+batch_size]\n","\n","            images = []\n","            targets = []\n","            for i in range(len(batch_samples)):\n","                batch_sample = batch_samples[i]\n","                batch_target = batch_targets[i]\n","                im = Image.open(batch_sample)\n","                cur_width = im.size[0]\n","                cur_height = im.size[1]\n","\n","                # print(cur_width, cur_height)\n","                height_fac = 113 / cur_height\n","\n","                new_width = int(cur_width * height_fac)\n","                size = new_width, 113\n","\n","                imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n","                now_width = imresize.size[0]\n","                now_height = imresize.size[1]\n","                # Generate crops of size 113x113 from this resized image and keep random 10% of crops\n","\n","                avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n","\n","                # Pick random x%\n","                pick_num = int(len(avail_x_points)*factor)\n","\n","                # Now pick\n","                random_startx = sample(avail_x_points,  pick_num)\n","\n","                for start in random_startx:\n","                    imcrop = imresize.crop((start, 0, start+113, 113))\n","                    images.append(np.asarray(imcrop))\n","                    targets.append(batch_target)\n","\n","            # trim image to only see section with road\n","            X_train = np.array(images)\n","            y_train = np.array(targets)\n","\n","            #reshape X_train for feeding in later\n","            X_train = X_train.reshape(X_train.shape[0], 113, 113, 1)\n","            #convert to float and normalize\n","            X_train = X_train.astype('float32')\n","            X_train /= 255\n","\n","            #One hot encode y\n","            y_train = to_categorical(y_train, num_classes)\n","            yield shuffle(X_train, y_train)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbjdjR8g3OLI","colab_type":"code","colab":{}},"source":["train_generator = generate_data(train_files, train_targets, batch_size=batch_size, factor = 0.3)\n","validation_generator = generate_data(validation_files, validation_targets, batch_size=batch_size, factor = 0.3)\n","test_generator = generate_data(test_files, test_targets, batch_size=batch_size, factor = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwhOwQUiAOn-","colab_type":"code","colab":{}},"source":["def resize_image(image):\n","    import tensorflow as tf\n","    return tf.image.resize(image,[128,32])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGdzDREVBs5R","colab_type":"code","colab":{}},"source":["# Function to resize image to 64x64\n","row, col, ch = 113, 113, 1\n","\n","model = Sequential()\n","model.add(ZeroPadding2D((1, 1), input_shape=(row, col, ch)))\n","\n","# Resise data within the neural network\n","model.add(Lambda(resize_image))  #resize images to allow for easy computation\n","#model.add(Lambda(lambda x: resize_image))\n","\n","# CNN model - Building the model suggested in paper\n","\n","model.add(Convolution2D(filters= 32, kernel_size =(5,5), strides= (2,2), padding='same', name='conv1')) #96\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool1'))\n","\n","model.add(Convolution2D(filters= 64, kernel_size =(3,3), strides= (1,1), padding='same', name='conv2'))  #256\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool2'))\n","\n","model.add(Convolution2D(filters=128, kernel_size =(3,3), strides= (1,1), padding='same', name='conv3'))  #256\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool3'))\n","\n","model.add(Bidirectional(LSTM('relu', dropout=0.5, return_sequences=True, time_major=False)))\n","\n","model.add(Dense(128, name='dense1'))  #1024\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(64, name='dense2'))  #1024\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(num_classes,name='output'))\n","model.add(Activation('softmax'))  #softmax since output is within 50 classes\n","print(\"Worked till here\")\n","\n","model.compile(loss='ctc_beam_search_decoder', optimizer=Adam(), metrics=['accuracy'])\n","\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bspauftXUvIr","colab_type":"code","colab":{}},"source":["# Training the Model\n","nb_epoch = 4\n","\n","samples_per_epoch = 658\n","nb_val_samples = 170\n","\n","\n","# #save every model using Keras checkpoint\n","from keras.callbacks import ModelCheckpoint\n","filepath=\"gdrive/My Drive/check-{epoch:02d}-{val_loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath= filepath, verbose=1, save_best_only=False)\n","callbacks_list = [checkpoint]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wds6paQD_K2R","colab_type":"code","colab":{}},"source":["# #Model fit generator\n","model.fit_generator(train_generator, steps_per_epoch= samples_per_epoch,\n","                                      validation_data=validation_generator,\n","                                      validation_steps=nb_val_samples, epochs=nb_epoch, verbose=1, callbacks=callbacks_list)\n","model.load_weights('drive/My Drive/check-04-6.6100.hdf5')#load the checkpoint whatever created, it will show at the end after completion of model.fit()\n","scores = model.evaluate_generator(test_generator,245) \n","print(\"Accuracy = \", scores[1]*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfar8AKMCwtF","colab_type":"code","colab":{}},"source":[" images = []\n","for filename in test_files[:50]:\n","  im = Image.open(filename)\n","  cur_width = im.size[0]\n","  cur_height = im.size[1]\n","  #print(\"Before Crop:\", cur_width, cur_height)\n","  height_fac = 113 / cur_height\n","  new_width = int(cur_width * height_fac)\n","  size = new_width, 113\n","  imresize = im.resize((size), Image.ANTIALIAS)  # Resize so height = 113 while keeping aspect ratio\n","  now_width = imresize.size[0]\n","  now_height = imresize.size[1]\n","  #print(\"After Crop:\", now_width, now_height)\n","  avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n","  # Pick random x%\n","  factor = 0.1\n","  pick_num = int(len(avail_x_points)*factor)\n","  #print(\"Pick Number is ::\", pick_num)\n","  random_startx = sample(avail_x_points,  pick_num)\n","  for start in random_startx:\n","    imcrop = imresize.crop((start, 0, start+113, 113))\n","    images.append(np.asarray(imcrop))\n","  X_test = np.array(images)\n","  X_test = X_test.reshape(X_test.shape[0], 113, 113, 1)\n","  #convert to float and normalize\n","  X_test = X_test.astype('float32')\n","  X_test /= 255\n","  shuffle(X_test)\n","print(X_test.shape)\n","\n","predictions = model.predict(X_test,verbose=1)\n","print(predictions.shape)\n","predicted_word = []\n","for pred in predictions:\n","     predicted_word.append(np.argmax(pred))\n","print(len(predicted_word))"],"execution_count":null,"outputs":[]}]}